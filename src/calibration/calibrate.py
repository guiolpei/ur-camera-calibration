#!/usr/bin/env python

import numpy as np 
import cv2
import json
import png
from robot import Robot
import time
from scipy import optimize  
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  
import utils

# User options (change me)
# --------------- Setup options ---------------
tcp_host_ip = '192.168.1.250' # IP and port to robot arm as TCP client
tcp_port = 30002
rtc_host_ip = '192.168.1.250' # IP and port to robot arm as real-time client
rtc_port = 30003
workspace_limits = np.asarray([[-0.101, 0.243], [-0.616, -0.234], [0.367, 0.458]]) # Cols: min max, Rows: x y z (define workspace limits in robot coordinates in meters)
calib_grid_step = 0.05
checkerboard_offset_from_base = np.array([[-0.095],[-0.471],[-0.076], [1]])
tool_orientation = [0.00,0.00,0.00]
checkerboard_size = (3,3)
# ---------------------------------------------

# Construct 3D calibration grid across workspace
gridspace_x = np.linspace(workspace_limits[0][0], workspace_limits[0][1], int(1 + (workspace_limits[0][1] - workspace_limits[0][0])/calib_grid_step))
gridspace_y = np.linspace(workspace_limits[1][0], workspace_limits[1][1], int(1 + (workspace_limits[1][1] - workspace_limits[1][0])/calib_grid_step))
gridspace_z = np.linspace(workspace_limits[2][0], workspace_limits[2][1], int(1 + (workspace_limits[2][1] - workspace_limits[2][0])/calib_grid_step))

calib_grid_x, calib_grid_y, calib_grid_z = np.meshgrid(gridspace_x, gridspace_y, gridspace_z)

num_calib_grid_pts = calib_grid_x.shape[0]*calib_grid_x.shape[1]*calib_grid_x.shape[2]

calib_grid_x.shape = (num_calib_grid_pts,1)
calib_grid_y.shape = (num_calib_grid_pts,1)
calib_grid_z.shape = (num_calib_grid_pts,1)

calib_grid_pts = np.concatenate((calib_grid_x, calib_grid_y, calib_grid_z), axis=1)

#measured_pts: points generated by sampling out of the workspace_limits[] + checkerboard offset from tool. 
#              It is the position of the tool when taking a picture, ideally this is the position of the center of the checkerboard in robot world coordinates.
#              This is achieved easily when the camera is fixed and the robot moves the checkerboard in the image.
#              As the robot position + checkerboard offset from tool = the position of the center of the fixed checkerboard in robot world coordinates.
measured_pts = []
#obseverved_pts: This is the position X,Y,Z in meters of the center of the checkerboard with respect to the origin of the camera in the camera world coordinates. 
observed_pts = []
#observed_pix: Pixel locations of the center of the checkerboard in the image.
observed_pix = []

print('Going to calibrate in : ', num_calib_grid_pts, 'different points.')

# Connect to the robot
print('Connecting to robot...')
robot = Robot(tcp_host_ip, tcp_port, rtc_host_ip, rtc_port)
# Slow down robot to SAFE values
robot.joint_acc = robot.JOINT_ACC_SAFE
robot.joint_vel = robot.JOINT_VEL_SAFE
#robot.go_photo()
robot.move_to(checkerboard_offset_from_base[:3, 0], tool_orientation)
# Move robot to each calibration point in workspace
print('Collecting data...')
for calib_pt_idx in range(num_calib_grid_pts):
    tool_position = calib_grid_pts[calib_pt_idx,:]
    print('good ', calib_pt_idx, ': ', tool_position, tool_orientation)
    robot.move_to(tool_position, tool_orientation)
    time.sleep(1)
    refine_criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
    # Wait for a coherent pair of frames: depth and color
    camera_color_img, camera_depth_img = robot.camera.get_data()
    if not (camera_depth_img is None and camera_color_img is None):
        bgr_color_data = cv2.cvtColor(camera_color_img, cv2.COLOR_RGB2BGR)
        gray_data = cv2.cvtColor(bgr_color_data, cv2.COLOR_BGR2GRAY)
        checkerboard_found, corners = cv2.findChessboardCorners(gray_data, checkerboard_size, None, cv2.CALIB_CB_ADAPTIVE_THRESH)
        if checkerboard_found:
            corners_refined = cv2.cornerSubPix(gray_data, corners, (3,3), (-1,-1), refine_criteria)
            # Get observed checkerboard center 3D point in camera space
            checkerboard_pix = np.round(corners_refined[4,0,:]).astype(int)
            checkerboard_z = camera_depth_img[checkerboard_pix[1]][checkerboard_pix[0]]
            checkerboard_x = np.multiply(checkerboard_pix[0]-robot.camera.intrinsics[0][2], checkerboard_z/robot.camera.intrinsics[0][0])
            checkerboard_y = np.multiply(checkerboard_pix[1]-robot.camera.intrinsics[1][2], checkerboard_z/robot.camera.intrinsics[1][1])
            if checkerboard_z != 0:
                # Get current robot position
                current_position = robot.get_current_pose()
                rot_vec = np.array(current_position)
                rot_vec.shape = (1,6)
                T_be = utils.V2T(rot_vec)
                invT_be = np.linalg.inv(T_be)

                # Save calibration point and observed checkerboard center
                checker2tool = np.dot(invT_be, checkerboard_offset_from_base) 
                checker2tool = checker2tool[:3, 0]
                #tool_position = checkerboard_offset_from_base - tool_position
                observed_pts.append([checkerboard_x, checkerboard_y, checkerboard_z])                    
                measured_pts.append(checker2tool)
                observed_pix.append(checkerboard_pix)
                # Save calibration point and observed checkerboard center
                print('Observed points: ', [checkerboard_x,checkerboard_y,checkerboard_z])
                print('Checkerboard pix: ', checkerboard_pix)
                print('Measured points: ',  checker2tool)
                # Draw and display the corners
                #vis = cv2.drawChessboardCorners(bgr_color_data, (1,1), corners_refined[4,:,:], checkerboard_found)
                #filename = 'images/calibrationPt' + str(calib_pt_idx) + '.png'
                #cv2.imwrite(filename, vis)
                #cv2.imshow('Calibration',vis)
                #cv2.waitKey(10)
            else:
                print('checkerboard Z == 0')
        else:
            #name = 'calibration point ' + str(calib_pt_idx) 
            #cv2.imshow(name,camera_color_img)
            #cv2.waitKey(1)
            print('No checkerboard found')
    else:
        print('No depth or color frames')

# Move robot back to photo pose
robot.go_photo()

measured_pts = np.asarray(measured_pts)
observed_pts = np.asarray(observed_pts)
observed_pix = np.asarray(observed_pix)
world2camera = np.eye(4)
print(measured_pts)
print(observed_pts)
print(observed_pix)
print('Total valid points: ',  measured_pts.shape[0], '/', num_calib_grid_pts)

# Estimate rigid transform with SVD
def get_rigid_transform(A, B):
    assert len(A) == len(B)
    N = A.shape[0]; # Total points
    centroid_A = np.mean(A, axis=0)
    centroid_B = np.mean(B, axis=0)
    AA = A - np.tile(centroid_A, (N, 1)) # Centre the points
    BB = B - np.tile(centroid_B, (N, 1))
    H = np.dot(np.transpose(AA), BB) # Dot is matrix multiplication for array
    U, S, Vt = np.linalg.svd(H)
    R = np.dot(Vt.T, U.T)
    if np.linalg.det(R) < 0: # Special reflection case
       Vt[2,:] *= -1
       R = np.dot(Vt.T, U.T)
    t = np.dot(-R, centroid_A.T) + centroid_B.T
    return R, t

def get_rigid_transform_error(z_scale):
    global measured_pts, observed_pts, observed_pix, world2camera

    # Apply z offset and compute new observed points using camera intrinsics
    observed_z = observed_pts[:,2:] * z_scale
    observed_x = np.multiply(observed_pix[:,[0]]-robot.camera.intrinsics[0][2],observed_z/robot.camera.intrinsics[0][0])
    observed_y = np.multiply(observed_pix[:,[1]]-robot.camera.intrinsics[1][2],observed_z/robot.camera.intrinsics[1][1])
    new_observed_pts = np.concatenate((observed_x, observed_y, observed_z), axis=1)

    # Estimate rigid transform between measured points and new observed points
    R, t = get_rigid_transform(np.asarray(measured_pts), np.asarray(new_observed_pts))
    t.shape = (3,1)
    world2camera = np.concatenate((np.concatenate((R, t), axis=1),np.array([[0, 0, 0, 1]])), axis=0)

    # Compute rigid transform error
    registered_pts = np.dot(R,np.transpose(measured_pts)) + np.tile(t,(1,measured_pts.shape[0]))
    error = np.transpose(registered_pts) - new_observed_pts
    error = np.sum(np.multiply(error,error))
    rmse = np.sqrt(error/measured_pts.shape[0]);
    return rmse

# Optimize z scale w.r.t. rigid transform error
print('Calibrating...')
z_scale_init = 1
optim_result = optimize.minimize(get_rigid_transform_error, np.asarray(z_scale_init), method='Nelder-Mead')
camera_depth_offset = optim_result.x

# Save camera optimized offset and camera pose
print('Saving...')
np.savetxt('camera_depth_scale.txt', camera_depth_offset, delimiter=' ')
get_rigid_transform_error(camera_depth_offset)
camera_pose = np.linalg.inv(world2camera)
np.savetxt('camera_pose.txt', camera_pose, delimiter=' ')
print('Done.')


# DEBUG CODE -----------------------------------------------------------------------------------
#np.savetxt('measured_pts.txt', np.asarray(measured_pts), delimiter=' ')
#np.savetxt('observed_pts.txt', np.asarray(observed_pts), delimiter=' ')
#np.savetxt('observed_pix.txt', np.asarray(observed_pix), delimiter=' ')
#measured_pts = np.loadtxt('measured_pts.txt', delimiter=' ')
#observed_pts = np.loadtxt('observed_pts.txt', delimiter=' ')
#observed_pix = np.loadtxt('observed_pix.txt', delimiter=' ')
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(measured_pts[:,0],measured_pts[:,1],measured_pts[:,2], c='blue')
print(camera_depth_offset)
R, t = get_rigid_transform(np.asarray(measured_pts), np.asarray(observed_pts))
t.shape = (3,1)
camera_pose = np.concatenate((np.concatenate((R, t), axis=1),np.array([[0, 0, 0, 1]])), axis=0)
camera2robot = np.linalg.inv(camera_pose)
t_observed_pts = np.transpose(np.dot(camera2robot[0:3,0:3],np.transpose(observed_pts)) + np.tile(camera2robot[0:3,3:],(1,observed_pts.shape[0])))
ax.scatter(t_observed_pts[:,0],t_observed_pts[:,1],t_observed_pts[:,2], c='red')
new_observed_pts = observed_pts.copy()
new_observed_pts[:,2] = new_observed_pts[:,2] * camera_depth_offset[0]
R, t = get_rigid_transform(np.asarray(measured_pts), np.asarray(new_observed_pts))
t.shape = (3,1)
camera_pose = np.concatenate((np.concatenate((R, t), axis=1),np.array([[0, 0, 0, 1]])), axis=0)
camera2robot = np.linalg.inv(camera_pose)
t_new_observed_pts = np.transpose(np.dot(camera2robot[0:3,0:3],np.transpose(new_observed_pts)) + np.tile(camera2robot[0:3,3:],(1,new_observed_pts.shape[0])))
ax.scatter(t_new_observed_pts[:,0],t_new_observed_pts[:,1],t_new_observed_pts[:,2], c='green')
plt.show()
